{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW: X-ray images classification\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, open Mobaxterm and connect to triton with the user and password you were give with. Activate the environment `2ndPaper` and then type the command `pip install scikit-image`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will be dealing with classification of 32X32 X-ray images of the chest. The image can be classified into one of four options: lungs (l), clavicles (c), and heart (h) and background (b). Even though those labels are dependent, we will treat this task as multiclass and not as multilabel. The dataset for this assignment is located on a shared folder on triton (`/MLdata/MLcourse/X_ray/'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, MaxPool2D, Conv2D, Dropout\n",
    "from tensorflow.keras.layers import Flatten, InputLayer\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import *\n",
    "from skimage.io import imread\n",
    "\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "config = tf.compat.v1.ConfigProto(gpu_options =\n",
    "                         tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8)\n",
    "# device_count = {'GPU': 1}\n",
    ")\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(datapath):\n",
    "    # This part reads the images\n",
    "    classes = ['b','c','l','h']\n",
    "    imagelist = [fn for fn in os.listdir(datapath)]\n",
    "    N = len(imagelist)\n",
    "    num_classes = len(classes)\n",
    "    images = np.zeros((N, 32, 32, 1))\n",
    "    Y = np.zeros((N,num_classes))\n",
    "    ii=0\n",
    "    for fn in imagelist:\n",
    "\n",
    "        src = imread(os.path.join(datapath, fn),1)\n",
    "        img = resize(src,(32,32),order = 3)\n",
    "        \n",
    "        images[ii,:,:,0] = img\n",
    "        cc = -1\n",
    "        for cl in range(len(classes)):\n",
    "            if fn[-5] == classes[cl]:\n",
    "                cc = cl\n",
    "        Y[ii,cc]=1\n",
    "        ii += 1\n",
    "\n",
    "    BaseImages = images\n",
    "    BaseY = Y\n",
    "    return BaseImages, BaseY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train_and_val(datapath):\n",
    "    # This part reads the images\n",
    "    classes = ['b','c','l','h']\n",
    "    imagelist = [fn for fn in os.listdir(datapath)]\n",
    "    N = len(imagelist)\n",
    "    num_classes = len(classes)\n",
    "    images = np.zeros((N, 32, 32, 1))\n",
    "    Y = np.zeros((N,num_classes))\n",
    "    ii=0\n",
    "    for fn in imagelist:\n",
    "\n",
    "        images[ii,:,:,0] = imread(os.path.join(datapath, fn),1)\n",
    "        cc = -1\n",
    "        for cl in range(len(classes)):\n",
    "            if fn[-5] == classes[cl]:\n",
    "                cc = cl\n",
    "        Y[ii,cc]=1\n",
    "        ii += 1\n",
    "\n",
    "    return images, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data for training and validation:\n",
    "src_data = 'MLdata/MLcourse/X_ray/'\n",
    "train_path = src_data + 'train'\n",
    "val_path = src_data + 'validation'\n",
    "test_path = src_data + 'test'\n",
    "BaseX_train , BaseY_train = preprocess_train_and_val(train_path)\n",
    "BaseX_val , BaseY_val = preprocess_train_and_val(val_path)\n",
    "X_test, Y_test = preprocess(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 1: Fully connected layers \n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 1:***</span> *NN with fully connected layers. \n",
    "\n",
    "Elaborate a NN with 2 hidden fully connected layers with 300, 150 neurons and 4 neurons for classification. Use ReLU activation functions for the hidden layers and He_normal for initialization. Don't forget to flatten your image before feedforward to the first dense layer. Name the model `model_relu`.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "input_shape = BaseX_train[0].shape\n",
    "model_relu = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(300, activation='relu', kernel_initializer=initializer),\n",
    "    tf.keras.layers.Dense(150, activation='relu', kernel_initializer=initializer),\n",
    "    tf.keras.layers.Dense(4)\n",
    "])\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               307500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4)                 604       \n",
      "=================================================================\n",
      "Total params: 353,254\n",
      "Trainable params: 353,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_relu.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model with the optimizer above, accuracy metric and adequate loss for multiclass task. Train your model on the training set and evaluate the model on the testing set. Print the accuracy and loss over the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 2.7868 - accuracy: 0.4126\n",
      "Epoch 2/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 5.2851 - accuracy: 0.5063\n",
      "Epoch 3/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 5.4206 - accuracy: 0.4972\n",
      "Epoch 4/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 4.5447 - accuracy: 0.4015\n",
      "Epoch 5/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 3.7627 - accuracy: 0.4430\n",
      "Epoch 6/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 3.4237 - accuracy: 0.4455\n",
      "Epoch 7/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 3.5954 - accuracy: 0.4442\n",
      "Epoch 8/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 3.0398 - accuracy: 0.3133\n",
      "Epoch 9/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 2.9445 - accuracy: 0.3193\n",
      "Epoch 10/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 3.4172 - accuracy: 0.4019\n",
      "Epoch 11/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 3.1038 - accuracy: 0.4154\n",
      "Epoch 12/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 2.9618 - accuracy: 0.4197\n",
      "Epoch 13/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 2.6547 - accuracy: 0.4419\n",
      "Epoch 14/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 2.3136 - accuracy: 0.4589\n",
      "Epoch 15/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 1.8776 - accuracy: 0.3454\n",
      "Epoch 16/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 1.6162 - accuracy: 0.3057\n",
      "Epoch 17/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 1.3151 - accuracy: 0.3539\n",
      "Epoch 18/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 1.3001 - accuracy: 0.3593\n",
      "Epoch 19/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 1.2919 - accuracy: 0.3681\n",
      "Epoch 20/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 1.2858 - accuracy: 0.3772\n",
      "Epoch 21/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 1.2702 - accuracy: 0.4005\n",
      "Epoch 22/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 1.2575 - accuracy: 0.4370\n",
      "Epoch 23/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 1.2495 - accuracy: 0.4561\n",
      "Epoch 24/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 1.2436 - accuracy: 0.4700\n",
      "Epoch 25/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 1.2396 - accuracy: 0.4788\n",
      "6/6 [==============================] - 0s 833us/step - loss: 1.2704 - accuracy: 0.3543\n",
      "test loss, test acc: [1.270440697669983, 0.35428571701049805]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "model_relu.compile(optimizer=AdamOpt,\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_relu.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "results = model_relu.evaluate(X_test,Y_test)\n",
    "print('test loss, test acc:', results)\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 2:***</span> *Activation functions.* \n",
    "\n",
    "Change the activation functions to LeakyRelu or tanh or sigmoid. Name the new model `new_a_model`. Explain how it can affect the model.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "input_shape = BaseX_train[0].shape\n",
    "new_a_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(300, activation='sigmoid', kernel_initializer=initializer),\n",
    "    tf.keras.layers.Dense(150, activation='sigmoid', kernel_initializer=initializer),\n",
    "    tf.keras.layers.Dense(4)\n",
    "])\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rectified Linear Unit (ReLU) convert linear outputs of a neuron into nonlinear outputs by outputting x for all x >= 0 and 0 for all x < 0. \n",
    "In other words, it equals max(x, 0). This simplicity makes it more attractive than the Sigmoid activation function and the Tangens hyperbolicus (Tanh) activation function, which use more difficult formulas and are computationally more expensive. \n",
    "In addition, ReLU is not sensitive to vanishing gradients, whereas the other two are, slowing down learning in your network and also known to generalize well.\n",
    "This does however not mean that ReLU itself does not have certain challenges:\n",
    "- It tends to produce very large values given its non-boundedness on the upside of the domain. Theoretically, infinite inputs produce infinite outputs.\n",
    "- If a neuron’s weights are moved towards the zero output, it may be the case that they eventually will no longer be capable of recovering from this. They will then continually output zeros. This is especially the case when our network is poorly initialized, or when your data is poorly normalized.\n",
    "- Small values, even the non-positive ones, may be of value; they can help capture patterns underlying the dataset. With ReLU, this cannot be done, since all outputs smaller than zero are zero.\n",
    "\n",
    "We prefer the Sigmoid function because it outputs between (0,1). When estimating a probability, this is perfect, because probabilities have a very similar range of [0,1]. Especially in binary classification problems, when we effectively estimate the probability that the output is of some class, Sigmoid functions allow us to give a very weighted estimate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               307500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 604       \n",
      "=================================================================\n",
      "Total params: 353,254\n",
      "Trainable params: 353,254\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_a_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 3:***</span> *Number of epochs.* \n",
    "\n",
    "Train the new model using 25 and 40 epochs. What difference does it makes in term of performance? Remember to save the compiled model for having initialized weights for every run as we did in tutorial 12. Evaluate each trained model on the test set*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 4.7579 - accuracy: 0.2502\n",
      "Epoch 2/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 5.3469 - accuracy: 0.2502\n",
      "Epoch 3/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 9.1151 - accuracy: 0.2502\n",
      "Epoch 4/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 9.6029 - accuracy: 0.2502\n",
      "Epoch 5/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 9.9712 - accuracy: 0.2502\n",
      "Epoch 6/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 10.2923 - accuracy: 0.2502\n",
      "Epoch 7/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 10.4392 - accuracy: 0.2502\n",
      "Epoch 8/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 10.4989 - accuracy: 0.2502\n",
      "Epoch 9/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 10.5786 - accuracy: 0.2502\n",
      "Epoch 10/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 10.6309 - accuracy: 0.2502\n",
      "Epoch 11/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 10.6857 - accuracy: 0.2502\n",
      "Epoch 12/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 10.7678 - accuracy: 0.2502\n",
      "Epoch 13/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 10.8375 - accuracy: 0.2502\n",
      "Epoch 14/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 10.9446 - accuracy: 0.2502\n",
      "Epoch 15/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 10.9993 - accuracy: 0.2502\n",
      "Epoch 16/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 17/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 18/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 19/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 20/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 21/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 22/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 23/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 24/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 25/25\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 12.0655 - accuracy: 0.2514\n",
      "test loss, test acc: [12.065545082092285, 0.2514285743236542]\n",
      "Saved trained model at results/25_epochs_final_weights.h5 \n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "new_a_model.compile(optimizer=AdamOpt,\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "new_a_model.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "results = new_a_model.evaluate(X_test,Y_test)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"25_epochs_final_weights.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "new_a_model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "#-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: \n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 0\n",
    "batch_size = 64\n",
    "epochs = 40\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 2/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 3/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 4/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 5/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 6/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 7/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 8/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 9/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 10/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 11/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 12/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 13/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 14/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 15/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 16/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 17/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 18/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 19/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 20/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 21/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 22/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 23/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 24/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 25/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 26/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 27/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 28/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 29/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 30/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 31/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 32/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 33/40\n",
      "102/102 [==============================] - 0s 2ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 34/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 35/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 36/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 37/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 38/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 39/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "Epoch 40/40\n",
      "102/102 [==============================] - 0s 1ms/step - loss: 11.0541 - accuracy: 0.2502\n",
      "6/6 [==============================] - 0s 996us/step - loss: 12.0655 - accuracy: 0.2514\n",
      "test loss, test acc: [12.065545082092285, 0.2514285743236542]\n",
      "Saved trained model at results/40_epochs_final_weights.h5 \n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "new_a_model.compile(optimizer=AdamOpt,\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "new_a_model.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "results = new_a_model.evaluate(X_test,Y_test)\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"40_epochs_final_weights.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "new_a_model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "#-----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 4:***</span> *Mini-batches.* \n",
    "\n",
    "Build the `model_relu` again and run it with a batch size of 32 instead of 64. What are the advantages of the mini-batch vs. SGD?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(mini-) Batch Gradient Descent involves calculations over the full training set at each step as a result of which it is very slow on very large training data. Thus, it becomes very computationally expensive to do Batch GD. However, this is great for convex or relatively smooth error manifolds. Also, Batch GD scales well with the number of features.\n",
    "\n",
    "SGD tries to solve the main problem in Batch Gradient descent which is the usage of whole training data to calculate gradients as each step. SGD is stochastic in nature, i.e, it picks up a “random” instance of training data at each step and then computes the gradient making it much faster as there is much fewer data to manipulate at a single time, unlike Batch GD.\n",
    "There is a downside of the Stochastic nature of SGD: once it reaches close to the minimum value then it doesn’t settle down, instead bounces around which gives us a good value for model parameters but not optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "input_shape = BaseX_train[0].shape\n",
    "model_relu = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(300, activation='relu', kernel_initializer=initializer),\n",
    "    tf.keras.layers.Dense(150, activation='relu', kernel_initializer=initializer),\n",
    "    tf.keras.layers.Dense(4)\n",
    "])\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.6209 - accuracy: 0.2660\n",
      "Epoch 2/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.2241 - accuracy: 0.2777\n",
      "Epoch 3/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.7193 - accuracy: 0.2575\n",
      "Epoch 4/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.4202 - accuracy: 0.2502\n",
      "Epoch 5/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.5240 - accuracy: 0.2550\n",
      "Epoch 6/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.0272 - accuracy: 0.3202\n",
      "Epoch 7/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.9772 - accuracy: 0.2967\n",
      "Epoch 8/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.8839 - accuracy: 0.2621\n",
      "Epoch 9/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.7142 - accuracy: 0.2502\n",
      "Epoch 10/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.2666 - accuracy: 0.2683\n",
      "Epoch 11/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1906 - accuracy: 0.3167\n",
      "Epoch 12/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.1633 - accuracy: 0.3375\n",
      "Epoch 13/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.7866 - accuracy: 0.3860\n",
      "Epoch 14/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.3246 - accuracy: 0.3973\n",
      "Epoch 15/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.0982 - accuracy: 0.3752\n",
      "Epoch 16/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.0112 - accuracy: 0.3681\n",
      "Epoch 17/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.9694 - accuracy: 0.3644\n",
      "Epoch 18/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.0732 - accuracy: 0.3631\n",
      "Epoch 19/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.2818 - accuracy: 0.3627\n",
      "Epoch 20/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.4558 - accuracy: 0.3627\n",
      "Epoch 21/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.6596 - accuracy: 0.3622\n",
      "Epoch 22/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.7167 - accuracy: 0.3619\n",
      "Epoch 23/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.7548 - accuracy: 0.3607\n",
      "Epoch 24/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.3911 - accuracy: 0.3636\n",
      "Epoch 25/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.2072 - accuracy: 0.4076\n",
      "Epoch 26/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.9336 - accuracy: 0.4050\n",
      "Epoch 27/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.0657 - accuracy: 0.3832\n",
      "Epoch 28/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.2739 - accuracy: 0.4325\n",
      "Epoch 29/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.5445 - accuracy: 0.4303\n",
      "Epoch 30/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.5841 - accuracy: 0.4353\n",
      "Epoch 31/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.9917 - accuracy: 0.4262\n",
      "Epoch 32/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.4156 - accuracy: 0.3826\n",
      "Epoch 33/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.8493 - accuracy: 0.3781\n",
      "Epoch 34/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.0072 - accuracy: 0.3865\n",
      "Epoch 35/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.8964 - accuracy: 0.4126\n",
      "Epoch 36/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.0215 - accuracy: 0.4178\n",
      "Epoch 37/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.0731 - accuracy: 0.3829\n",
      "Epoch 38/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.2708 - accuracy: 0.3245\n",
      "Epoch 39/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.6772 - accuracy: 0.2618\n",
      "Epoch 40/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.1268 - accuracy: 0.2502\n",
      "Epoch 41/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 5.0942 - accuracy: 0.2502\n",
      "Epoch 42/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.5391 - accuracy: 0.2502\n",
      "Epoch 43/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 7.9561 - accuracy: 0.2502\n",
      "Epoch 44/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 8.7696 - accuracy: 0.2502\n",
      "Epoch 45/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.2598 - accuracy: 0.2502\n",
      "Epoch 46/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.5982 - accuracy: 0.2502\n",
      "Epoch 47/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.7426 - accuracy: 0.2502\n",
      "Epoch 48/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 9.8553 - accuracy: 0.2502\n",
      "Epoch 49/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 6.3161 - accuracy: 0.2893\n",
      "Epoch 50/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 4.4353 - accuracy: 0.3128\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 5.0281 - accuracy: 0.1143\n",
      "test loss, test acc: [5.028085708618164, 0.11428571492433548]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "model_relu.compile(optimizer=AdamOpt,\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_relu.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "results = model_relu.evaluate(X_test,Y_test)\n",
    "print('test loss, test acc:', results)\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 5:***</span> *Batch normalization.* \n",
    "\n",
    "Build the `new_a_model` again and add batch normalization layers. How does it impact your results?*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "input_shape = BaseX_train[0].shape\n",
    "new_a_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=input_shape),\n",
    "    tf.keras.layers.Dense(300, activation='sigmoid', kernel_initializer=initializer),\n",
    "    tf.keras.layers.BatchNormalization(axis=1),\n",
    "    tf.keras.layers.Dense(150, activation='sigmoid', kernel_initializer=initializer),\n",
    "    tf.keras.layers.BatchNormalization(axis=1),\n",
    "    tf.keras.layers.Dense(4)\n",
    "])\n",
    "#---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "#Define your optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "#Compile the network: \n",
    "new_a_model.compile(optimizer=AdamOpt,\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 4.5452 - accuracy: 0.2394\n",
      "Epoch 2/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 3.1007 - accuracy: 0.2618\n",
      "Epoch 3/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 2.9592 - accuracy: 0.2607\n",
      "Epoch 4/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.5001 - accuracy: 0.2359\n",
      "Epoch 5/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 2.5152 - accuracy: 0.2388\n",
      "Epoch 6/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 2.4063 - accuracy: 0.2505\n",
      "Epoch 7/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.5225 - accuracy: 0.2488\n",
      "Epoch 8/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.5246 - accuracy: 0.2739\n",
      "Epoch 9/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.4773 - accuracy: 0.2878\n",
      "Epoch 10/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 2.4154 - accuracy: 0.2884\n",
      "Epoch 11/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.3169 - accuracy: 0.2939\n",
      "Epoch 12/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 2.4603 - accuracy: 0.2933\n",
      "Epoch 13/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 2.1884 - accuracy: 0.2966\n",
      "Epoch 14/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.1695 - accuracy: 0.2909\n",
      "Epoch 15/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.2029 - accuracy: 0.2967\n",
      "Epoch 16/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 2.1864 - accuracy: 0.3017\n",
      "Epoch 17/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.1517 - accuracy: 0.2922\n",
      "Epoch 18/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.1580 - accuracy: 0.2773\n",
      "Epoch 19/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 2.0963 - accuracy: 0.2652\n",
      "Epoch 20/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.1538 - accuracy: 0.2179\n",
      "Epoch 21/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.1790 - accuracy: 0.2587\n",
      "Epoch 22/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.2157 - accuracy: 0.2458\n",
      "Epoch 23/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.6174 - accuracy: 0.2281\n",
      "Epoch 24/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.5768 - accuracy: 0.2339\n",
      "Epoch 25/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.4739 - accuracy: 0.2442\n",
      "Epoch 26/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 2.3129 - accuracy: 0.2348\n",
      "Epoch 27/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 2.3458 - accuracy: 0.2377\n",
      "Epoch 28/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.1884 - accuracy: 0.2400\n",
      "Epoch 29/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.1827 - accuracy: 0.2657\n",
      "Epoch 30/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.1210 - accuracy: 0.2695\n",
      "Epoch 31/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.1126 - accuracy: 0.2672\n",
      "Epoch 32/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 2.4396 - accuracy: 0.2573\n",
      "Epoch 33/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.5131 - accuracy: 0.2620\n",
      "Epoch 34/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.3940 - accuracy: 0.2519\n",
      "Epoch 35/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 2.4396 - accuracy: 0.2584\n",
      "Epoch 36/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.4097 - accuracy: 0.2765\n",
      "Epoch 37/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.3825 - accuracy: 0.2706\n",
      "Epoch 38/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 2.3653 - accuracy: 0.3153\n",
      "Epoch 39/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.2835 - accuracy: 0.3233\n",
      "Epoch 40/50\n",
      "203/203 [==============================] - 0s 2ms/step - loss: 2.2386 - accuracy: 0.3244\n",
      "Epoch 41/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.2426 - accuracy: 0.3251\n",
      "Epoch 42/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.2897 - accuracy: 0.3179\n",
      "Epoch 43/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.2401 - accuracy: 0.3187\n",
      "Epoch 44/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.2206 - accuracy: 0.3085\n",
      "Epoch 45/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.3036 - accuracy: 0.2884\n",
      "Epoch 46/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.2834 - accuracy: 0.2817\n",
      "Epoch 47/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.2912 - accuracy: 0.2861\n",
      "Epoch 48/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.2813 - accuracy: 0.2929\n",
      "Epoch 49/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.2294 - accuracy: 0.2969\n",
      "Epoch 50/50\n",
      "203/203 [==============================] - 0s 1ms/step - loss: 2.2559 - accuracy: 0.3044\n",
      "6/6 [==============================] - 0s 998us/step - loss: 3.0571 - accuracy: 0.3200\n",
      "test loss, test acc: [3.057131290435791, 0.3199999928474426]\n"
     ]
    }
   ],
   "source": [
    "#Preforming the training by using fit \n",
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "new_a_model.fit(BaseX_train, BaseY_train, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "results = new_a_model.evaluate(X_test,Y_test)\n",
    "print('test loss, test acc:', results)\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2: Convolutional Neural Network (CNN)\n",
    "------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 1:***</span> *2D CNN.* \n",
    "\n",
    "Have a look at the model below and answer the following:\n",
    "* How many layers does it have?\n",
    "* How many filter in each layer?\n",
    "* Would the number of parmaters be similar to a fully connected NN?\n",
    "* Is this specific NN performing regularization?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_net(input_shape,drop,dropRate,reg):\n",
    "    #Defining the network architecture:\n",
    "    model = Sequential()\n",
    "    model.add(Permute((1,2,3),input_shape = input_shape))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_1',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_2',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:    \n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_3',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_4',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_5',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    #Fully connected network tail:      \n",
    "    model.add(Dense(512, activation='elu',name='FCN_1')) \n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(Dense(128, activation='elu',name='FCN_2'))\n",
    "    model.add(Dense(4, activation= 'softmax',name='FCN_3'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 1e-03\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "drop = True\n",
    "dropRate = 0.3\n",
    "reg = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 32, 32, 64)        640       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 16, 16, 128)       64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "Conv2D_5 (Conv2D)            (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 256)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "FCN_1 (Dense)                (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FCN_2 (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "FCN_3 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 3,271,492\n",
      "Trainable params: 3,271,332\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NNet=get_net(input_shape,drop,dropRate,reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "1. How many layers does it have?\n",
    "\n",
    "    The following network has 24 layers\n",
    "\n",
    "2. How many filters in each layer?\n",
    "\n",
    "    Conv2D_1 has 64 filters, Conv2D_2 and Conv2D_3 have 128 filters, Conv2D_4 and Conv2D_5 have 256 filters\n",
    "\n",
    "3. Would the number of parmaters be similar to a fully connected NN?\n",
    "\n",
    "    Num of parmaters in a Conv layer = ((shape of width of the filter * shape of height of the filter * number of filters in the previous layer+1)*number of filters)\n",
    "    \n",
    "    Num of parmaters in a Fully connected = ((current layer neurons * previous layer neurons)+1*c)\n",
    "\n",
    "    As we can cealry see, the number of parmaters of a fully connected NN would increas dramatically.\n",
    "\n",
    "4. Is this specific NN performing regularization?\n",
    "\n",
    "    This specific NN performs L2 regularization with value of 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import *\n",
    "import os\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n",
    "#Compile the network: \n",
    "NNet.compile(optimizer=AdamOpt, metrics=['acc'], loss='categorical_crossentropy')\n",
    "\n",
    "#Saving checkpoints during training:\n",
    "Checkpath = os.getcwd()\n",
    "Checkp = ModelCheckpoint(Checkpath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, save_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "102/102 [==============================] - 37s 362ms/step - loss: 7.9929 - acc: 0.4476 - val_loss: 7.8973 - val_acc: 0.2500\n",
      "Epoch 2/25\n",
      "102/102 [==============================] - 37s 362ms/step - loss: 7.6147 - acc: 0.5436 - val_loss: 8.0183 - val_acc: 0.2500\n",
      "Epoch 3/25\n",
      "102/102 [==============================] - 37s 362ms/step - loss: 7.4466 - acc: 0.5763 - val_loss: 8.1156 - val_acc: 0.2697\n",
      "Epoch 4/25\n",
      "102/102 [==============================] - 37s 361ms/step - loss: 7.3275 - acc: 0.6166 - val_loss: 8.1497 - val_acc: 0.2951\n",
      "Epoch 5/25\n",
      "102/102 [==============================] - 37s 360ms/step - loss: 7.2402 - acc: 0.6361 - val_loss: 8.0580 - val_acc: 0.2766\n",
      "Epoch 6/25\n",
      "102/102 [==============================] - 37s 361ms/step - loss: 7.1534 - acc: 0.6673 - val_loss: 7.8777 - val_acc: 0.3247\n",
      "Epoch 7/25\n",
      "102/102 [==============================] - 38s 370ms/step - loss: 7.1051 - acc: 0.6784 - val_loss: 7.7426 - val_acc: 0.3530\n",
      "Epoch 8/25\n",
      "102/102 [==============================] - 38s 371ms/step - loss: 7.0485 - acc: 0.6889 - val_loss: 7.6371 - val_acc: 0.4068\n",
      "Epoch 9/25\n",
      "102/102 [==============================] - 38s 368ms/step - loss: 6.9947 - acc: 0.7098 - val_loss: 7.6214 - val_acc: 0.4034\n",
      "Epoch 10/25\n",
      "102/102 [==============================] - 37s 366ms/step - loss: 6.9440 - acc: 0.7200 - val_loss: 7.5753 - val_acc: 0.4039\n",
      "Epoch 11/25\n",
      "102/102 [==============================] - 39s 383ms/step - loss: 6.8985 - acc: 0.7326 - val_loss: 7.5924 - val_acc: 0.4161\n",
      "Epoch 12/25\n",
      "102/102 [==============================] - 39s 381ms/step - loss: 6.8669 - acc: 0.7348 - val_loss: 7.5806 - val_acc: 0.3964\n",
      "Epoch 13/25\n",
      "102/102 [==============================] - 39s 379ms/step - loss: 6.8342 - acc: 0.7403 - val_loss: 7.5728 - val_acc: 0.3958\n",
      "Epoch 14/25\n",
      "102/102 [==============================] - 39s 378ms/step - loss: 6.8043 - acc: 0.7436 - val_loss: 7.5461 - val_acc: 0.4005\n",
      "Epoch 15/25\n",
      "102/102 [==============================] - 37s 368ms/step - loss: 6.7663 - acc: 0.7552 - val_loss: 7.5437 - val_acc: 0.3953\n",
      "Epoch 16/25\n",
      "102/102 [==============================] - 37s 364ms/step - loss: 6.7186 - acc: 0.7666 - val_loss: 7.5207 - val_acc: 0.3981\n",
      "Epoch 17/25\n",
      "102/102 [==============================] - 39s 382ms/step - loss: 6.6873 - acc: 0.7712 - val_loss: 7.5175 - val_acc: 0.4028\n",
      "Epoch 18/25\n",
      "102/102 [==============================] - 39s 378ms/step - loss: 6.6655 - acc: 0.7793 - val_loss: 7.4682 - val_acc: 0.4126\n",
      "Epoch 19/25\n",
      "102/102 [==============================] - 38s 372ms/step - loss: 6.6484 - acc: 0.7779 - val_loss: 7.4920 - val_acc: 0.4062\n",
      "Epoch 20/25\n",
      "102/102 [==============================] - 38s 368ms/step - loss: 6.6147 - acc: 0.7892 - val_loss: 7.4474 - val_acc: 0.4086\n",
      "Epoch 21/25\n",
      "102/102 [==============================] - 38s 369ms/step - loss: 6.5805 - acc: 0.7955 - val_loss: 7.4488 - val_acc: 0.4115\n",
      "Epoch 22/25\n",
      "102/102 [==============================] - 38s 368ms/step - loss: 6.5466 - acc: 0.7994 - val_loss: 7.4462 - val_acc: 0.4091\n",
      "Epoch 23/25\n",
      "102/102 [==============================] - 38s 368ms/step - loss: 6.5407 - acc: 0.7977 - val_loss: 7.4555 - val_acc: 0.4028\n",
      "Epoch 24/25\n",
      "102/102 [==============================] - 38s 368ms/step - loss: 6.5137 - acc: 0.8055 - val_loss: 7.4250 - val_acc: 0.4109\n",
      "Epoch 25/25\n",
      "102/102 [==============================] - 38s 369ms/step - loss: 6.4743 - acc: 0.8134 - val_loss: 7.4325 - val_acc: 0.4120\n"
     ]
    }
   ],
   "source": [
    "#Preforming the training by using fit \n",
    "# IMPORTANT NOTE: This will take a few minutes!\n",
    "h = NNet.fit(x=BaseX_train, y=BaseY_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "#NNet.save(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# NNet.load_weights('Weights_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 28ms/step - loss: 7.9439 - acc: 0.3029\n",
      "test loss, test acc: [7.943883419036865, 0.3028571307659149]\n"
     ]
    }
   ],
   "source": [
    "results = NNet.evaluate(X_test,Y_test)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:red\">***Task 2:***</span> *Number of filters* \n",
    "\n",
    "Rebuild the function `get_net` to have as an input argument a list of number of filters in each layers, i.e. for the CNN defined above the input should have been `[64, 128, 128, 256, 256]`. Now train the model with the number of filters reduced by half. What were the results.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_1 (Permute)          (None, 32, 32, 1)         0         \n",
      "_________________________________________________________________\n",
      "Conv2D_1 (Conv2D)            (None, 32, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "Conv2D_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 16, 16, 64)        64        \n",
      "_________________________________________________________________\n",
      "Conv2D_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 16, 16, 64)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "Conv2D_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 8, 8, 128)         32        \n",
      "_________________________________________________________________\n",
      "Conv2D_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 8, 8, 128)         32        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "FCN_1 (Dense)                (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "FCN_2 (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "FCN_3 (Dense)                (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,392,772\n",
      "Trainable params: 1,392,612\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "102/102 [==============================] - 18s 174ms/step - loss: 4.9701 - acc: 0.3859 - val_loss: 4.7145 - val_acc: 0.2500\n",
      "Epoch 2/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 4.5839 - acc: 0.4892 - val_loss: 4.8141 - val_acc: 0.2500\n",
      "Epoch 3/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 4.4698 - acc: 0.5213 - val_loss: 4.8332 - val_acc: 0.2523\n",
      "Epoch 4/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 4.3458 - acc: 0.5525 - val_loss: 4.8013 - val_acc: 0.2593\n",
      "Epoch 5/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 4.2556 - acc: 0.5842 - val_loss: 4.7282 - val_acc: 0.3351\n",
      "Epoch 6/25\n",
      "102/102 [==============================] - 17s 170ms/step - loss: 4.2316 - acc: 0.5901 - val_loss: 4.6511 - val_acc: 0.3860\n",
      "Epoch 7/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 4.1789 - acc: 0.6019 - val_loss: 4.5980 - val_acc: 0.4144\n",
      "Epoch 8/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 4.1356 - acc: 0.6248 - val_loss: 4.5693 - val_acc: 0.4097\n",
      "Epoch 9/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 4.0913 - acc: 0.6245 - val_loss: 4.5579 - val_acc: 0.4138\n",
      "Epoch 10/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 4.0568 - acc: 0.6421 - val_loss: 4.5457 - val_acc: 0.4248\n",
      "Epoch 11/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 4.0330 - acc: 0.6464 - val_loss: 4.5439 - val_acc: 0.4190\n",
      "Epoch 12/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 4.0225 - acc: 0.6483 - val_loss: 4.5452 - val_acc: 0.4080\n",
      "Epoch 13/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 3.9950 - acc: 0.6557 - val_loss: 4.5490 - val_acc: 0.4115\n",
      "Epoch 14/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 3.9710 - acc: 0.6668 - val_loss: 4.5478 - val_acc: 0.4132\n",
      "Epoch 15/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 3.9533 - acc: 0.6749 - val_loss: 4.5459 - val_acc: 0.4057\n",
      "Epoch 16/25\n",
      "102/102 [==============================] - 18s 175ms/step - loss: 3.9161 - acc: 0.6852 - val_loss: 4.5505 - val_acc: 0.4080\n",
      "Epoch 17/25\n",
      "102/102 [==============================] - 17s 172ms/step - loss: 3.8907 - acc: 0.6903 - val_loss: 4.5546 - val_acc: 0.4057\n",
      "Epoch 18/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 3.8865 - acc: 0.6881 - val_loss: 4.5437 - val_acc: 0.4103\n",
      "Epoch 19/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 3.8689 - acc: 0.6915 - val_loss: 4.5477 - val_acc: 0.4016\n",
      "Epoch 20/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 3.8575 - acc: 0.6985 - val_loss: 4.5475 - val_acc: 0.4022\n",
      "Epoch 21/25\n",
      "102/102 [==============================] - 18s 172ms/step - loss: 3.8505 - acc: 0.6969 - val_loss: 4.5514 - val_acc: 0.4022\n",
      "Epoch 22/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 3.8250 - acc: 0.7071 - val_loss: 4.5565 - val_acc: 0.4010\n",
      "Epoch 23/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 3.8105 - acc: 0.7150 - val_loss: 4.5579 - val_acc: 0.3976\n",
      "Epoch 24/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 3.7918 - acc: 0.7241 - val_loss: 4.5648 - val_acc: 0.3976\n",
      "Epoch 25/25\n",
      "102/102 [==============================] - 17s 171ms/step - loss: 3.7914 - acc: 0.7206 - val_loss: 4.5640 - val_acc: 0.3953\n",
      "6/6 [==============================] - 0s 13ms/step - loss: 5.0214 - acc: 0.2571\n",
      "test loss, test acc: [5.021355628967285, 0.2571428716182709]\n"
     ]
    }
   ],
   "source": [
    "#--------------------------Impelment your code here:-------------------------------------\n",
    "def get_net(input_shape,drop,dropRate,reg, filters):\n",
    "    #Defining the network architecture:\n",
    "    model = Sequential()\n",
    "    model.add(Permute((1,2,3),input_shape = input_shape))\n",
    "    model.add(Conv2D(filters=filters[0], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_1',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=filters[1], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_2',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:    \n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=filters[2], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_3',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(filters=filters[3], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_4',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(Conv2D(filters=filters[4], kernel_size=(3,3), padding='same', activation='relu',name='Conv2D_5',kernel_regularizer=regularizers.l2(reg)))\n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(BatchNormalization(axis=1))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    #Fully connected network tail:      \n",
    "    model.add(Dense(512, activation='elu',name='FCN_1')) \n",
    "    if drop:\n",
    "        model.add(Dropout(rate=dropRate))\n",
    "    model.add(Dense(128, activation='elu',name='FCN_2'))\n",
    "    model.add(Dense(4, activation= 'softmax',name='FCN_3'))\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "input_shape = (32,32,1)\n",
    "learn_rate = 1e-5\n",
    "decay = 1e-03\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "drop = True\n",
    "dropRate = 0.3\n",
    "reg = 1e-2\n",
    "filters = np.array([64, 128, 128, 256, 256])\n",
    "filters = filters/2\n",
    "\n",
    "NNet=get_net(input_shape,drop,dropRate,reg,filters)\n",
    "\n",
    "from tensorflow.keras.optimizers import *\n",
    "import os\n",
    "from tensorflow.keras.callbacks import *\n",
    "\n",
    "#Defining the optimizar parameters:\n",
    "AdamOpt = Adam(lr=learn_rate,decay=decay)\n",
    "\n",
    "#Compile the network: \n",
    "NNet.compile(optimizer=AdamOpt, metrics=['acc'], loss='categorical_crossentropy')\n",
    "\n",
    "#Saving checkpoints during training:\n",
    "Checkpath = os.getcwd()\n",
    "Checkp = ModelCheckpoint(Checkpath, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, save_freq=1)\n",
    "\n",
    "#Preforming the training by using fit \n",
    "# IMPORTANT NOTE: This will take a few minutes!\n",
    "h = NNet.fit(x=BaseX_train, y=BaseY_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_split=0, validation_data = (BaseX_val, BaseY_val), shuffle=True)\n",
    "\n",
    "results = NNet.evaluate(X_test,Y_test)\n",
    "print('test loss, test acc:', results)\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all folks! See you :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
